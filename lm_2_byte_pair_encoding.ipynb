{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "6f1b1d5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "bbd658f0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'int'>, {'_ I': 1, '_ l i k e': 1, '_ t o': 1, '_ w a t c h': 1, '_ g o o d': 1, '_ m o v i e s': 1})\n",
            "{'m', 'k', 'h', 'I', 's', 'l', 'e', 'v', 'a', 'i', 'd', 'c', 'o', 'w', 'g', '_', 't'}\n"
          ]
        }
      ],
      "source": [
        "def initialize_vocabulary(corpus):\n",
        "    vocabulary = defaultdict(int)\n",
        "    charset = set()\n",
        "    for word in corpus:\n",
        "        word_with_marker = '_' + word\n",
        "        characters = list(word_with_marker)\n",
        "        charset.update(characters)\n",
        "        tokenized_word = \" \".join(characters)  # Use space to separate tokens\n",
        "        vocabulary[tokenized_word] += 1\n",
        "    return vocabulary, charset\n",
        "corpus = [\"I\", \"like\", \"to\", \"watch\", \"good\", \"movies\"]  # [\"I\", \"like\", \"to\", \"watch\", \"bad\", \"movies\"], [\"I\", \"like\", \"to\", \"watch\", \"good\", \"movies\", \"with\", \"my\", \"friends\"]\n",
        "vocabulary, charset = initialize_vocabulary(corpus)\n",
        "print(vocabulary)\n",
        "print(charset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "aa941b25",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocabulary: defaultdict(<class 'int'>, {'_ I': 1, '_ l i k e': 1, '_ t o': 1, '_ w a t c h': 1, '_ g o o d': 1, '_ m o v i e s': 1})\n",
            "pair_counts: defaultdict(<class 'int'>, {('_', 'I'): 1, ('_', 'l'): 1, ('l', 'i'): 1, ('i', 'k'): 1, ('k', 'e'): 1, ('_', 't'): 1, ('t', 'o'): 1, ('_', 'w'): 1, ('w', 'a'): 1, ('a', 't'): 1, ('t', 'c'): 1, ('c', 'h'): 1, ('_', 'g'): 1, ('g', 'o'): 1, ('o', 'o'): 1, ('o', 'd'): 1, ('_', 'm'): 1, ('m', 'o'): 1, ('o', 'v'): 1, ('v', 'i'): 1, ('i', 'e'): 1, ('e', 's'): 1})\n"
          ]
        }
      ],
      "source": [
        "def get_pair_counts(vocabulary):\n",
        "    pair_counts = defaultdict(int)\n",
        "    for tokenized_word, count in vocabulary.items():\n",
        "        tokens = tokenized_word.split()\n",
        "        for i in range(len(tokens) - 1):\n",
        "            pair = (tokens[i], tokens[i+1])\n",
        "            pair_counts[pair] += count\n",
        "    return pair_counts\n",
        "print(f\"vocabulary: {vocabulary}\")\n",
        "print(f\"pair_counts: {get_pair_counts(vocabulary)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "5f8cd49d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocabulary: defaultdict(<class 'int'>, {'_ I': 1, '_ l i k e': 1, '_ t o': 1, '_ w a t c h': 1, '_ g o o d': 1, '_ m o v i e s': 1})\n",
            "pair_counts: defaultdict(<class 'int'>, {('_', 'I'): 1, ('_', 'l'): 1, ('l', 'i'): 1, ('i', 'k'): 1, ('k', 'e'): 1, ('_', 't'): 1, ('t', 'o'): 1, ('_', 'w'): 1, ('w', 'a'): 1, ('a', 't'): 1, ('t', 'c'): 1, ('c', 'h'): 1, ('_', 'g'): 1, ('g', 'o'): 1, ('o', 'o'): 1, ('o', 'd'): 1, ('_', 'm'): 1, ('m', 'o'): 1, ('o', 'v'): 1, ('v', 'i'): 1, ('i', 'e'): 1, ('e', 's'): 1})\n",
            "new_vocabulary: defaultdict(<class 'int'>, {'_ I': 1, '_ l i ke': 1, '_ t o': 1, '_ w a t c h': 1, '_ g o o d': 1, '_ m o v i e s': 1})\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def merge_pair(vocabulary, pair):\n",
        "    new_vocabulary = defaultdict(int)\n",
        "    bigram = re.escape(' '.join(pair)) # escape function automatically adds backslashes to special characters in string (like .,*?) so they are interpreted as literal characters\n",
        "    pattern = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\") # matches only whole token pairs; bigram is not part of a word (e.g. good morning in thisisgood morning doesn't match)\n",
        "    for tokenized_word, count in vocabulary.items():\n",
        "        new_tokenized_word = pattern.sub(\"\".join(pair), tokenized_word) # replace all occurrences of matched pattern with the joined pair\n",
        "        new_vocabulary[new_tokenized_word] += count\n",
        "    return new_vocabulary\n",
        "\n",
        "print(f\"vocabulary: {vocabulary}\")\n",
        "print(f\"pair_counts: {get_pair_counts(vocabulary)}\")\n",
        "print(f\"new_vocabulary: {merge_pair(vocabulary, ('k', 'e'))}\")\n",
        "# Note\n",
        "# _ l i k e: 1 -> _ l i ke: 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "2226b5f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17\n",
            "vocabulary: defaultdict(<class 'int'>, {'_I': 1, '_like': 1, '_to': 1, '_w a t c h': 1, '_ g o o d': 1, '_ m o v i e s': 1})\n",
            "merges: [('_', 'I'), ('_', 'l'), ('_l', 'i'), ('_li', 'k'), ('_lik', 'e'), ('_', 't'), ('_t', 'o'), ('_', 'w')]\n",
            "charset: {'m', 'k', 'h', 'I', 's', 'l', 'e', 'v', 'a', 'i', 'd', 'c', 'o', 'w', 'g', '_', 't'}\n",
            "tokens: {'m', '_lik', 'h', '_I', '_li', '_t', 'i', 'o', '_w', 'g', 't', 'k', 'I', 's', 'l', 'e', '_like', 'v', 'a', '_l', 'd', 'c', '_to', 'w', '_'}\n"
          ]
        }
      ],
      "source": [
        "# BPE algorithm = iteratively merges most frequent pair of tokens until vocab_size is reached\n",
        "def byte_pair_encoding(corpus, vocab_size):\n",
        "    vocabulary, charset = initialize_vocabulary(corpus)\n",
        "    merges = []\n",
        "    tokens = set(charset)\n",
        "    print(len(tokens))\n",
        "    while len(tokens) < vocab_size:\n",
        "        pair_counts = get_pair_counts(vocabulary)\n",
        "        if not pair_counts:\n",
        "            break\n",
        "        most_frequent_pair = max(pair_counts, key=pair_counts.get)\n",
        "        merges.append(most_frequent_pair)\n",
        "        vocabulary = merge_pair(vocabulary, most_frequent_pair)\n",
        "        new_token = ''.join(most_frequent_pair) \n",
        "        tokens.add(new_token)\n",
        "    return vocabulary, merges, charset, tokens\n",
        "\n",
        "# vocab_size must be larger than initial character count (17)\n",
        "# BPE grows vocabulary by merging pairs, so we need a target larger than starting size\n",
        "vocabulary, merges, charset, tokens = byte_pair_encoding(corpus, 25)\n",
        "print(f\"vocabulary: {vocabulary}\")\n",
        "print(f\"merges: {merges}\")\n",
        "print(f\"charset: {charset}\")\n",
        "print(f\"tokens: {tokens}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "fe725585",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['_', '<unk>', 'e', 't', 's', '<unk>', '<unk>', '<unk>', 'o', 'c', 'e', 'e', 'd', '<unk>', 't', 'o', '<unk>', 't', 'h', 'e', '<unk>', 'l', 'a', '<unk>', 'g', '<unk>', 'a', 'g', 'e', '<unk>', 'm', 'o', 'd', 'e', 'l', 'i', '<unk>', 'g', '<unk>', 'c', 'h', 'a', '<unk>', 't', 'e', '<unk>']\n"
          ]
        }
      ],
      "source": [
        "def tokenize_word(word, merges, vocabulary, charset, unk_token='<unk>'):\n",
        "    word = '_' + word\n",
        "    if word in vocabulary:\n",
        "        return vocabulary[word]\n",
        "    tokens = [char if char in charset else unk_token for char in word]\n",
        "    \n",
        "    for left, right in merges:\n",
        "        i = 0\n",
        "        while i < len(tokens) - 1:\n",
        "            if tokens[i: i+2] == [left, right]:\n",
        "                tokens[i: i+2] = [left + right]\n",
        "            else:\n",
        "                i += 1\n",
        "    return tokens\n",
        "\n",
        "tokenized_word = tokenize_word('Lets proceed to the language modeling chapter', merges, vocabulary, charset)\n",
        "print(tokenized_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "563c3d7c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
