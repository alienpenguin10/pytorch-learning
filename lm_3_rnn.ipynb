{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7659b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30de858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmanRNNUnit(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.Uh = nn.Parameter(torch.randn(embedding_dim, embedding_dim)) # Note: Not nn.Linear\n",
    "        self.Wh = nn.Parameter(torch.randn(embedding_dim, embedding_dim)) # weight matrices for the hidden state and input vector with random values\n",
    "        self.b = nn.Parameter(torch.zeros(embedding_dim)) # bias vector to zero\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        # x: (batch_size, embedding_dim)\n",
    "        # h: (batch_size, embedding_dim)\n",
    "        return torch.tanh(matmul(x, self.Uh) + matmul(h, self.Wh) + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab433f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmanRNN(nn.Module):\n",
    "    def __init__(self, emb_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_units = nn.ModuleList([ElmanRNNUnit(emb_dim) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, embedding_dim)\n",
    "        # x = document\n",
    "        # seq_len = max num of words in a document\n",
    "        # embedding_dim = size of the word embedding\n",
    "        batch_size, seq_len, emb_dim = x.shape\n",
    "        h_prev = [torch.zeros(batch_size, emb_dim, device=x.device) for _ in range(self.num_layers)]\n",
    "        outputs = []\n",
    "        for time_step in range(seq_len):\n",
    "            x_t = x[:, time_step, :] # (batch_size, emb_dim)\n",
    "            for layer_idx, rnn_unit in enumerate(self.rnn_units):\n",
    "                h_new = rnn_unit(x_t, h_prev[layer_idx]) # Update hidden state\n",
    "                h_prev[layer_idx] = h_new # Input for next layer\n",
    "                input_t = h_new # Collect outputs\n",
    "            outputs.append(input_t)\n",
    "        return torch.stack(outputs, dim=1) # (batch_size, seq_len, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99cc3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
